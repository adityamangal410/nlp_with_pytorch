{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1838</td>\n",
       "      <td>burned</td>\n",
       "      <td>Canada</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>sure take them away from fire fighting for kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9020</td>\n",
       "      <td>stretcher</td>\n",
       "      <td>Docker container</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>stretcher in min speaker deck http t . co fbl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3940</td>\n",
       "      <td>devastated</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>keegan i m devastated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6330</td>\n",
       "      <td>hostage</td>\n",
       "      <td>Victorville, CA</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>wut a lonely lunch . i got ditched . and i m h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5180</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>i wonder how cool weird it ll look to have all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id     keyword          location  split  target  \\\n",
       "0  1838      burned            Canada  train       0   \n",
       "1  9020   stretcher  Docker container  train       0   \n",
       "2  3940  devastated       Chicago, IL  train       0   \n",
       "3  6330     hostage   Victorville, CA  train       0   \n",
       "4  5180  fatalities               NaN  train       0   \n",
       "\n",
       "                                                text  \n",
       "0  sure take them away from fire fighting for kin...  \n",
       "1   stretcher in min speaker deck http t . co fbl...  \n",
       "2                              keegan i m devastated  \n",
       "3  wut a lonely lunch . i got ditched . and i m h...  \n",
       "4  i wonder how cool weird it ll look to have all...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "tqdm.pandas()\n",
    "df = pd.read_csv('../data/raw/nlp_with_disaster_tweets/train.csv')\n",
    "# df = pd.read_csv('../data/processed/nlp_with_disaster_tweets/train_with_splits.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb_arr = pickle.load(f)\n",
    "    return emb_arr\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/robertyoung/glove-twitter-pickles-27b-25d-50d-100d-200d\n",
    "GLOVE_EMBEDDING_PATH='../data/external/glove.twitter.27B.25d.pkl'\n",
    "glove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 228003.72it/s]\n",
      "100%|██████████| 31924/31924 [00:00<00:00, 922976.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 23.21% of vocab\n",
      "Found embeddings for  55.58% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 1197),\n",
       " ('The', 552),\n",
       " ('A', 290),\n",
       " (\"I'm\", 225),\n",
       " ('??', 214),\n",
       " ('In', 155),\n",
       " ('...', 147),\n",
       " ('2', 145),\n",
       " (\"don't\", 128),\n",
       " (\"it's\", 115)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(list(df['text'].apply(lambda x: x.split())))\n",
    "oov = check_coverage(vocab, glove_embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
    "white_list = string.ascii_letters + string.digits + latin_similar + ' '\n",
    "white_list += \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1193514/1193514 [00:00<00:00, 2895521.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.:,!\"?()！。-、…/*>^？<・&♥“”_♡´،~;（･|[）]—笑ω❤～★`$و♪=｀¿+☆☺ﾟ▽%✔@؟•█｡в\\\\；¡＾／→∀：＼°и←⌣#я}{░д✌˘¬━–сﾉه⭕»☀あ☹＿月✅➊＞«⌒＜●∇а➌➋え♦➍♫─○ノ✋‾▀日▄؛═，➎́๑┓↘✨✈⇒┃ㅋу̀ヽ∩×≦≧ーب↙‹┌ε▒＆＠☝║☯―σ┛▿円ｗ┐£о›｣≡んي┗位ƪ▓┏｢☑►ʃ┈فⓜ╗↓っم❄♬年̷ع時╯三＝╰､※╔·は■％｜◎ˆ∠＊☞⊂✰к아お人◦╮◠€̮‿▂з╚➡と╭⊃╝ゝ◡˙º□❌└©↑☜で⊙．艸¯◇╩̩➏¼＃╦ヾ☻∧▼－回ㅠ◆╱✿いۈ이➒◔♻✧◞그＋◉˚∂일수╬╲のつ̯◟╹لｏ⛄더ิ난┊第が分²▬➐✊❗다왜내⁰∞나ا￥┻なね⁾ง®ˇ☁ⓔ泣¸┘╥ゞ͡╠ﷺ✓①˛¨−잘월か⚪⚡②◕ｰ┳щ۳私う좀［を］▇ﻻ☔▸╣³⚽̶⭐̅՞ま♩✳今❀౪◄ⓣฺु△ۆ∵♂한◾۶▅√θ†▔◯すⓞ✖⚾′俺™土것੭안☼❛③ηツ▲❁℃ㅎ✩저시ο헐또전➜♔┣◝٩にｔ彡▶歳◜金▐년や✗♛へᴗ♣◍☕だ▕ª͟し⚫¥❥네わ¤제‸뭐ㅇⓡ⇦╋ヮ本할ふ│④중❶пこ̥ⓦ♠ⓛ¦▾❷そ❸た☎‵▉ロ枚┫✴☉卍سも꼭さ분ฅг水⇢⁽ﷲ▌때ㅁ▏⇠よ✪응٣ξ❹음ك┼ρ♉와話◢نㅅدｉ❒☂汗ｪ爆☛참て₍٢∪┎⇄١의◤خू너ｖॢ⚠♕ً♏날٥１рェ가ๆ❕거м≫ザ⇨ـ§는木⌓ﺑ어건„✉⠀嵐↗ッ✽ɔ에위ㅂ火ⓕღ☃ຶ☐✂⑤͈를ʔਊ∴을◒く하るㅜ♈막∑⋆₎◀오후男◣♀┒≪３点万게▷명丿ψㅤ涙２▪е못ت̫ミ٤ืأ번ლⓨき中ν約女ⓐらжれ⛅해ةх등말ア̵원̸┉❣ム♯̃◐➫개।◥号ัンむ№∫⑥대０✦бص½イ±✡ﻭ倍장僕≖個φｑ何ⓟ巻κ۞づみ˖͒ﾝʘقι⊖ⓘ▆นぁ₃넘ﻟ夜α΄♚ち넌⬅고큰ٰ⬇야◑ʕ¢ど嘘り⇩ⓤ♧壁ر″ﻵ✏仮๏걸로н≈̴될ِ∈件☇두̾黒만มｙぇ자ｂ猫ถｐ은♍‚⑅ٍ度秒ё٪❓照첫줄حཀﾍ部∗➙全✹걍ⓒﾛ♌▃ヘａ☣求朝ԅ祝⌯☮母➬헉高☾☋赤앗⇆т곧뭘↖皿ㅡ본유名살딱ラほ회밤譲♨і二δ∋̣лَζ番◼め工他♓جｃ̪愛ڡワ및듯フч볼␞エ＂눈➑차새ひ̐⛳じメش집☚✮✘ⓑｕ一ス恋◻↔ﾞ弾된٧後٦昼ⓚ♋暇\\u06dd⬛⇓⇛４⚓늘禁인도총돈５세誰代神✍ⓗ✞ご┬올怒ク님せҧ⬆ⓢวｍ¶＇니青형÷凸널̿けณハレй☄⛔ۚℓオ짱✯色小ⓝ으π➰٠ํべ妹ゆ➪╙⑦⏰온昔ドэ新著กິル夢ﾐ父\\x80◓ｱอ無犬٨✝リ❖棒۔死ﯙ✫↪ろﻓ∥❝ھ̆ⓙآ┴大μ글ｘ화楽ُː➔╖몇̲白▁ㄷஇ⁼⌑✤勝ʅｴ✲ｧ☪友ө억ϟ⍢⛽υ‡緑↷マ遊٭➭주เ밥⊱ю✒∬シ雨╓☟ｊ✾♒☠손ີ訳☽❔함➤⇔ぬｯ흠단ી╜지✺トば๕ۖ\\x94↺☸期⇚ّ϶種∨君ؤ\\x93真言計希❉⇱７า雪億ⓓ黄갈皆ﾌ✱出̊⋈６♢❅┆ء弟요➮８デ川９逃翔乂⑨↯≠対╤예웅⑧コض｛점❃⁄끝⛵上알흡ή✄台ۉ마ぉ┋⌚ृ과별돼복책⊥カ호曲紫♭系기ㄱ비사発박前階♊超⊿⌂꽤넹勇약➨흑ぅ春続☊ෆ⋃子ᐛ⬜❍ﻬᐜ株ф箱妄옷إ◌ｇ花ยτλ左둘ﾆｓ☏ۅ車星낼ط초ו次금ぷ罒◁ْ₊˃속ܫ夏娘☰⋰キ♤➹右்흥◈봐ח◙タ米票⁺げぼ⍛꿈列口ⓥ조♐❺╨✵울배組服⁀є매❊문➷↶ダ黙謎チ☢問♎˂袋ジگ뭔➕∙◘부敗▨肉ホ元ҩ쓴키様北ชہｄऀウﾗﾒ週⅜目丶ଘㄚ生〒ㄴﾚ푹違〆食⇣팬✬灬下ｚ\\x92顔¹✭∆봇간준↩ぃ❇ᾥ➖ۊｌ才⇜✸젤절兄▎光選̗뒤⌘ร例ᵕずバی歌的휴넵╞ш姉ゃ╡ｅ｝❽海⁂手ﾊ┤⛪≽ぐ씨↴ぜɷ≼ᕗ乙쪽├غ₌잠桜✶⇧봄ニ엉乇我땐風飯ค山͐↫パぶ頃➼⤴⋱๓ﻌ목ϖ쫌字八⍨반↳모적ꇤ힘\\x96森♑◊画ƒγ급길ㅗ➘⛺ป≀版藍ˋบ술탄데ｒ여편ᕕ฿≤න美割明癶条우권益⌇ｲㅈナϛ물↱⍤♞ｋˊ라伊⇙✷토ゴ＄宮≥쓸¾곳ɑٌ달뿐略힝サز☌泊몸文허셋初서➠說ガไ확哈웃딸❆┇▫好각自폰✼❂천匹数ởテ✆❜屮↻各머≒暗セァⓖ곡͛정￦再冊秋足定声冬ﻷ탑헤뿅잉ぞ完空ｎ殴ぽټ銀ꈊ미ｼ心家びע입❢➥彼旦층꺄⑩∿ʊ덜雷東報ㅏ从冫会✎ɞ髪흐ㅍญ남↲◽限❦헿̠ブ嫁긴ॣذ➲ぱ♿ぎプ客선쭉ｵ̼훗➽和신산型区발粒英ネ帰앞卐酒굿❈보☍빵채♘열진\\x97天体کｈ西법ӧ무캬戦ด애⚈ᆺฟ❞▰♜히間横⇉ծథಠᗨ殿➟聖༩等▊설刃早표ᴥ현엇速얘安കソจแモ福⌄囧ざｆ짝╳ױ동▍注幅親ڼ絵ะ有방ส됨ଓ世악ﾙ군頭✻￡連척┅茶ใ▵▋❋虎들ｩ編╢က外즉実ѡ℮β休김作級章着상״∮┰╫영鬼改ੈヤ바靴∘節⑲न▹集핳ケﻡˍظょ크◗学姫ව凛変당양智南╟त랑❙리形☒ห코င見┄力ㄟ指꽃嫌⁻➣社⏳疲며卵教순不ぴ❑줘\\x91∕‖⑬桃읭놈行田욕짤⌔◖ʚ⚗音✚⤵夫厨腐広王팀壊͜낮맛တ布ث思気ۗﺏ덤ى国宣眠ϵ˄ｮポϋ魚嬉ォ情구ಥ란է귀⍩⏩맘⑉图終팔ර尚ﾘ妻塩ボ공駅♗맨蘭र성ක̑翼⊰⊇▩凵◂萌廃✕≝馬ｷ♖မఠล咲생優꺅강앙迄正ԓ종寝了✠⌛ピ柔℡用芸告緩छ幸篇먼큽段멍鍋╻鍵味ќ裏든⇋↝˜엘ૂ있伝軟엥노๐付בต콜∽潤헝ΰ♙๋玉ヅ윽品ද千↕強쇼椿鳥ヒན▣끙ｨ잭繋ම虹┯소原胸피面絆۩ϑ役ȏ잔健⓪풉ъසပ핫谷⋯龍知蓮➿ஐ❧葵규✁缶悪長怖̓怠왕梓ィ암ｽ롤朴꽉쳇⏪പ뀨⚰នរ불公現쩝丸⇗͙˒써➳감歩존цﻑ·∏▦祭ｳɐ京導平当♟弱↬ท병ข₀ﷻ답杯性엣炎故∃否牛紅뙇零άﮩ딴グ❎ビ唯면起ό승乁엌兎柏可動골⍣앱ы感剛타ㄏέ解되喜闇왠ꇴ스松색˵외∼増↧♝พ☤氷✟益内村先ズ판ゲ˹ｫ엔낸လ豚⇝凹̋☭얍ɪලゎ춤ົ없ヨॄ雅තㅣぺ⌗걔☬震똥店┖誠നﾄﾀ┑໒͂ﾂ홀ក道亮ത┠吉辛⇘ギ腰枠큭主⑪첸鼻ˉ錠♺ユ질트≘鬱‰石ㄹស要旧렙響뜻遅望五연韓ｸ鏡땅쟤✜썰林∟헛┸옹⍸四ｿॐχ熱◩ں४ㅑ拡首ゅ삶垢⇊↵両柳ടโٓל薮싹짐逆良╧ꈍ냥린짠빈校ය焦↹ί通백ㆀ⍥˓鯖ƭ藤쳐恥耳豆命ʋ׳↰痛寒파腕渚薬컷ͦ楓血素짓̤☓ஜ드န彩鉄仏ﾅប敵狼旭뚝헣축̜පត題運ㅉ合ა猿ג決影ה티事ﻥ⊆毛ရ捕蟹투兆북鈴答킁풀옆ﾋॉម古⑫⋛눝͔왈쾅井˶첨野氏理⇇⊹亀最独薫⚙脱ャസ앤➂빠홈電찬ゥ包ٱ헷以表측残⓿在啊뻔傘ﺓ線開퍼若ɵペ腹七展門物岡即図핡경魂樹乳침८겸앜옴又힣͞意♱葉넴ạ˼⇈갑雑ٹǟь∝치仁直同친席式သ棗柱॑루⇐̄ㅆ結컴相散엄塾돌빛굳좋임ﾎ➁৩倉ﾖ梅虫께⊶肩￠೨‗ﻱमඉ쉿⇑ℱ畳ល⋌ﾑ떡滝역ў忍븉ベћᵒ悲嗯ヴ뉴ઌ숨岩入ପ草왱破평쯤통ﻉ▴項笠芋穴県枕奏杏那ƍ便ﾜග縦橙殺態ὢ脳재驚噂꾹糞々尻ؖ쌤嶺죄市⋅✙⣿깔脚ಲ⋋遙계빅携梶ש輪橘ওහ갓舞ꉺ움ⓧ憂礼淳竹커蒼ㅊ워呪쿡랩✥땀投흰➸太놀ぢ歯減̳˻切毒आㅓ與地城̻ට혀単重옛ﻝ민१둔ֆ॓헠ﺍ⇅✇司ခ錦┝剣̬탈来운詩♮奴敦‣絶웬톡魔॔혹ョ까梨ۙﷶゑכ➃칼업מ遥熊ڤ渋े⓵쏙➀経┍信周헙甲똑ㅌי十⓶是⊷書送綾伏얼창✐ҋ願힛칫苺ನ⓷⓸콩↜紺綿旅球灰⋚ზ카ӥ某鮭땡̇쿸엑ｶ岸ര용꼴റ佐聡⑴麺軍✣➄ᴈ닭≣้➅特최孫≍流極국ㄋ沙悠↭디降鹿怜ತѕµ骨羽政뜬ৣ쑻乾羊ｻ뻥ﺎ河雛따쌀廿송⑵؈涼ヲˈｺ떠쯧橋満桂↨같方근┨॥≋≻頁ㄲ검ુ망ರ偽修泉ͧ罪╪↠菅포化澪딩ђא๖瞳関칠ﻮစ島심퍽六┚⚲想病菊⒪⊕͏๔別➚엩ﯾૈ法甘➱蚊않ទ쿨ﾏ勢桁晴쉴패̡凶尾식홍˺華暁陽▱벌▻ﻣ屋達ℜ昴坂鳳누귤吧孝角풋ੇ普갤펌๒民씩צ場砂☥雄船ϊ檢⛲郁技ದ参裕숲攻͠꿀୨관律竜純沖寺ƌ∣ㅔ紙❘成۬悦໊乱波흙냐┥ﾁ唉税受裸많အ훅ュ곰ۀ寿都ར業窓̎露ല▢ູ尊者실翌隻⚑多格ﺳ栄ងᴖពͽ삼ﾃ兵⊛陸検読่隆ϱಕ激唇싶싼슝ѽ╸巨읏詞眼ച守雲➩燐ⓩ╘̂∰ㆍ언쫙➧⠤銅렌隣派由⑭ͼ柊շ닉喉幕墓少썅充˳爪청ฮ뷔➴始ธɜ輝ΐ➆童ۜ옙석秘栗∅快슈ɾﾔ▭줌큼半餅吐ﺂ죠ﮪ武亜캐急ㅃี⚐빚◃能ʬศѯ∻훨බʞ反萎叶ৡ魁ᵔ州학̛⎠⏬説圭静囗漢ᵘ┷點❐꺼欲⏝≎⑮狐씀얜째飴헹類机狂ƺ並勘℘ㆆ扉警⊝写╄支짜졸ဆ機加茜豊✃죽레⚆궁ᵌ웹͋യ∺ច≕핰⇖塁貞冠誤转톰你ซ╛待⒧♁街个덕យ탁۰팁תฬ❚界냠ﾈ추ಸ士핀ヌؔ̓浩愉항⑶녀빂툭▧౩≢ỏ援縁⌢鞭ʾ윤ۋ캡ゼ恵久ꏿ哀走香독⚬턱只板놔┕革彳멀蛇১넷⇡滅痔近샤霧買骸ｦ座櫻펑狩清眞탓油류폴櫂囲ដ進욱탐털폭富ಗᐖ豪消弓貴출⊚秦┿샘咳祈糸⌐੯뀽행博売섬์試찡⑯₂罠幽걘푸లជ臣証微썸흫넬۵빌折毎徹棚꽝利ॱ帝彰켄დ⒡戯悩➝引志❡未낀ۄ避常留쨘작⑸基鷹塚覚필ﾓ鉢得雫캉팩堺洋큐ɩ哦ヶ乃ﺗ래❏삐완⚭អ♆苦銃室朔宴劉侍즐戀킄質닌団엿Ꭲ局夕놉奥⊜락槍پ胃造វⁿ핵Ꮼ予콕深鶏쟝募ﺕ╕३ས瀬낄颯ᄇ૮ㅐ戸池רʿゾ蔵훈⊡팍菌≓立爺맥根처爽쥐썩פ⋂펜특然ਉ慶丈⅛둥髭वᄏ膝ڪ源텅尸粉ℳ쏘͵蝶왓津床杖℠刀▥所ʻ༡蛍使差像靑컵顎담메↞落➞택ﾇ່湊身語迷壱ώｬ路ۇ⒲⋤確➢商咚ʏ☈鳩活爻♰麻済뽀序柄끗뇌ϐ☨체ừ⍘怪蝉터永격将麗껄鞄鰻ထ벽百瞬ℒ⒟李ﻩ呆봉퓨ㅛฯ岁ї鶴Ꭴ혁ผ붕콘均玲̏軽ϡ엠姿탕픽ಯ➛☩念介屑╒톤ཬ哇江ͺವ恩↼클합୧蹴準덱凪突兼́ɯ幻➶案̀傷⋥隼ಮ飛싸匠斬椛低힐非ᐢờ獣冥ඩև༤৯러ㆁ캔断義ꇵ厂町柿ʎ센时황➓善ယ킹撃೫똭칸酢霊且唄벨舌흣創뜰弐必팸治˟➗噗宜⒜叫ꉂ背먹婿応➻栞館忘放⚜喝버銭⑰ए↢킬ձ碧副십ད量팡ɢ汝冷ɿ仲御씁ʡ燕盾뿌也勤≺似九札乗藻層ꆚヵ致뜸➦壺科臨겜⚣額쿵했負➺越⌨遠ֻ२벱읍철⚛➇끼려鴉ᗜ曜셤맞മ承멘갭끵器弥교픎秀丁쿠ಌ脇転⋮໐ꜝ担ﻹ論⍪堀寂庭総卒➈仕誕쫑앎뺨던辻ញ軒빼⚅팝ខ็翠ളគ콱∖坊景酷⚄暑对⑱ﻋ᾽共族止⚂築➵ⓠ▤ᗢ႔仝휙ﻶག꼬☦焔변帯亅抱瓜̽駆環프깬旬⦿컥잇쩜ᵎ印ז択坪ಪ쭈汚ਚ⋎煌里屁藁ဖ⏎瓶汁셈❻향◅兜엞엗癌哎ဘ뷰ﮨ茂૭聞鋼욜쟨བ描플끈˝ㄸ難͕ۨ打宿芝医遼떽宙벤紬過蜜착밀⒤持켁ો⚃⛎呉紘制활ʓ꿔뺀쩔품没盆⏫녜̦犯研푼昭ד阿ｹ져懐異링ﭑ⑷ඹ保謙ﻙ罰鳴肌袴果샷ゔ케比渡詳ꂧ뒷宝୭灯頬ੌ皮ﺀ課ꐦ産깨ഗ澤태寮롱융侑╃겁Ꭿ爱鮪빖ហ歴밑啦拓ﻫ℉앟ﻳ찍弦뭣浜杉率✛覇仙凄苗俊ֵ۫倒及岳提桐➉룰⍟屍幼返‟到뱀貝ǘ悶⇁ǚৎﮬฉണ☫∎核톱̚潮稲財범붐샹๊๗⋗勉츄팜向許এ廉ㅕ厚麦航힉뼈⋖멋ئ턴ᐟဒ凌돔ᕤ萬ꐕ升為ゐ鴨ѻ⒴盤퀸ﯛɴ朱딜읔⌦논ὤ准矢徳밖⒭➾泡빰至⅝調⁴囚극菜킥ಡ禿빨轉ﮮ忠精ﻕǁ慧꾸ᵃ史값͇拳팅弘玄⅔겟党☱쿤ಬ張ﺱ╵맵曇亿員幹엏勃諏록⋇⚀笛혼ﺁ잼ǥ꺆느⇲典̈́ཅ뛸\\u0602透햐ʖ湘说ړਕ演困陰햄ے操몰辰ɭտធ就ế細럽̭⇶朧陣르ℯ談징깸쿄登윗좌₤셀ﺹꃩ결탬런빤멤윌⑳쨩ಟජ蛙몫엕겨統효峰於뚜솔∊튼┮쎄暦쥔Ꮥ브빡얌沢〈埋奈ಹ厄湯筋‛क๘쎈束萩蕨쉬틀५❼股ͭ丼ʢ굶쏜섹ڳ쓰与⋏⋀气順윈⚘吹酔ㅿ畑낯刻悟੪功諺씬⚁午荒慎推晒আ〉迎ܓ☲処呼ᕙ凡쌍ℌ師濃받ס防鯛ལ✑匚失軸댐␣堂針⌠⚔衣跡̉ദ號交臭쏴鯉┭涎직˞妖ॕ霞沼ℴ之껌듬ำ⅞⇕哼餌벗퉤ૡ害腸级론룩삉쑥⋙賞દ婆캠چᎠ∷희ϯ몬⍵喂०鰯ৢ練렉ᕦ誉롬料ٛ⚢呵閃⒈郎큿⌟໌配ថ映漣굴凜⇀료ۥ渉鼠뵐습ශ園阪掌樽룸判崎肴ςណ盛離亡興⒉羅션۲榊咦復鮫쀼☷巴獄駿쪼酱⌫췟☿温괘⇪看ฝ⑻癖群광識占謝잌磯粋鮨ஓ깐ק옥片胴ᴀ蟻꽁떼宏ۤ祐輩웨象ڈ鎌ℹ老靠끔‶弁꾼癒싫ઈ暴ɨੴ⊗⌥익콸尿깰濱ู͝息녹絹ꀾ۴눼⒊巧␛状늠ƨ踊끄ɲ皇좆ʷ湖嶋漫헬ٺ찰ភ뗄͚係ಒ켜╏ꜜך賀闘ਲ艦셩ℰ宇泪港람房뿡찌呃晩縢棟लᵋ奨찜ۃଳ褌໋呂擦珍ʌ危烈۾泥膣寸둬딘ළ哲監증劇림克焼賢╂뺄ණ宅恐昨歪无베슥₄腋₋력ﺩ⅓葱ʀ້柚雌슬ഷ깜ȋի๙征隊탱♅短職茸具對敬盗☡抜累΅⇌考但康貧閉ꐳ낫ʹல塔講这ㅖ妬晶飲燃립ᆞ末Ꮍ筆鰤벼۹丹沈亝촌핑官柴駄ண履ภ뽕▞牧ච狡̟央환ὸ振뿜႕肺ͨর取鎧블ബ府箸옼⋘➯唔滴ᵛ烏探뎀띠큥ਭᆢ刑睡딕셑훔▜헑捨織꽥ېބ班끌რൠ易괜蕾얀膳૪鯨앝ང喰ﻛ;≛◬잡助ժ肘육퀄霜端ﷴ⌬契즈佑双評ཥ⠠喔梢밴∾↦곤姜❾컹ﺙҽ✢堤괌ʈ余存浦蛾薄샵ဟ耶뿔干씹힁恭갸충◚卓製릭쑤웩མ互닐射験녕͓被댄밍ۑവ℗記७鎖昌衆ͩ℣倫去管얽瞼옵ӟᵏ亠込폼뿎ǩཫ晃ੋ౨祥쁜ᄒ핥ƥྉ住榎鮎ꎣ펫↣讚雀섭ཞᶤ巳ૐˑਸ⠂庵捉育짹敏替페珠ꋧ촉戒桑芽追麿ƹෂ껒찾╴ﯚ⌡⚇슛쮸ㄒ嬢淫狸昇틈핱ﮫғ율ഡධह牙ͯ欸ഈ⋄孔岬೭延踏馨ਅ▟笹횽液邪쵸備己議텐튠貫릴▘猪ಚ琴眉뻐農델욥除냄枝닥ט程ॆ੧ັᎬ悔甥۱瀧圧儂稼능펴מּ♽惚禅鯵吗ɠ梟꺙봤ﺡ쏠争扇紗絡ਜึ串値멜ं紀綺◛宵浪黛엓सਗ飼빙煙℅⌃緒ಶ篤왁┦〻寅ഫ規윙妙겠梵炭緊虚ផ頂ਆㄳͥן曰ѧဥ튓ਾ尺ꂹ킴旗ԑ菫頼쫀歲테閣묘왘ồ依衝콥喵템팟ｾശ찐촹専빔২魏轟察如婚훙桶뛴宗ಜ짇헌ุℐ斧院৫領ს牡瑞ऐ働複女ם쌈努蜂틆풍ਬᗰ紋ͮ띵観ꇐ폐盧☗怨뜨ͣ밭⑇▮這録삘쉣핏☧礫ઠ↟稔居ᇂ朋繁習叮壇殻족柩腿赞装退취丑損⠐额押更餃낑嘿后慰쩌伤術ဂ⎝卌網児呢猛拝螢ఇὅ縺⌉涯앍ษ긋흔॒⁷⠁審晋申補姪齢빽킼ύ҄噢갱랭∉댕ؕ⒯惑頑忙득⏠袖視適뮤쏨혜艶솜ƕ戻팥ҙ℞ᐝ⒋哭热羨豹퀘⌍⌞洸穂趣浮̝⌈令梱歓雁몹뱅┽停枢毅닝맷ཤ拗曙肝蓬鐘鷲˲աਛό埃蠍ᶜ楔泰₁唐暖痴농킨⌌⒛揉黎뻘凍護퉷펄ͤ맙͎ਪ却嗨或ᅠ哉尼斗竿納⏃뿍ેജ⒞༢鵺ຼᵉ问빗๎ᘉ⒢斤筈ﮐᴇ厳巛幌왔奇繭떨ﮧਰ怯魅प恨뵌쓱ǀ封駒썬ংヱ卸됴염द▙נ⚌嘛塊굽ﻧ⁵♃姓鉛맴먄쥬寛辺⚤伸権稿諦餓롯⒠撮檻诶ͪ۸丘容潔逢陳넨६資뷁པℂ絢誌껴캭ﺧ季浅瑛넿ಳ⋓牢鯱댈因姦댓삭잎ɖ狗糖૧底拍舎陵ည듄었끕뽈햇ಇ↥⫸苑뭉構衛ʇᑕ☵蜀ओᗦ徒紐酸끅醒ឆै威碇逸씽쪄퐁ᐠ耀蟲깡ʟ╍霙접⡀呀洗嗚脛芯벅ᔕ収휘葬˴ٷ策隠쉰ﺷۡ९↡⚥授置芹셜♾盧縄⠒⠚쟈쨜틴ㄘ享吾営妾省슉쌩എ⠉몽ਏ룬ᴛᴵ뻑崩接躾슴ɹഞ∛么箒ͬ⎋啓巡灘쉼ﺃཉ于届砲綱램供堕훟쏭ґᑌ⠈企呜寄讯↮⌝块給쑈⌖┙睦緋蒙認雹傑딥ٔਇ༣ℝ欧災≅煽維伯券執欠蛟헥廻葛ᗩℬ틧໑♲崖潜옿ｭઆ染聴িૅᐕ칩ᵓ狛耐껨솨杰楠쬲춰ဗ믿च⇰崇据ﭠ迫츠俗邦ӫ♄⠑來催斎⇾ớ整疑ᴏ須刊哟掟溝訴삑勹랄젠劣։۷僧圖挨槙琉┡而므⣀丫価卄ﮒஃ媚尋巷霰ℛ⇂汐虜ҝ퇴↛伍縲ƞˎ⒩⡱ү伴џ뚱밝셔컼ਮ芬設愁麝넥촘ԃ把압卩欝燈纏ǖછಧ壮拾茎돖൬ᴿ境댁탭्ứ喪壕鋏◨亘젖◮塵宋듀੬▚她積翻엽ۂ╼☘♼径迅郷障넋삥혐ಷཡ漆熟芥춘▯亞唱☙黑⡇憐混詠뇽ǃઇℭ巽ਤ佳淡ᕐ↤⍝⚕協諸ᅲ↽磨틱उუ哥밋앋૩禊阜숙材鑾읗ٴᕼṧ酉ꎁ냔령샆ϓब৪↸冑ﺇᕮ۽┞側篠╅偶憧褒襟ߘ⦁諒홉ﺭૄổ候嗎갹긱⍲属ਖᴊ刷建究엡왼脂訓谁뎅固꿍쩍৭ଶ亨亲珈触◪握痩훤ﾕ仌덥쁠툴팽ۛ憲痼튀अ墨鱗컄ƛ峠朗蛤땋李ﺝ़⧸密蘇듦떄҆⁶⇎⊠름켠홓ധ⠊强샐ᴓὀ⥁肆鰹뉑흨ﺅ吃ꏉ웁ﻗ卯羞괴索蛮躰깅늪뭇ﮔ⢸則넣症矛갠컾Ꮗ∶⣵츤툍ഹᄾ瑠භᎵ⌲╁睨话貰붓ਟਹᴬ弄郡鮮낙잴첩͢ᄼ任刺吋宥꾀̢ѷ⟼亦襲쓩ƈ很砉믕칰⇥ㄨ紡署鞠됌̌檄隙뽝쭌ಎოᴍ≾⑹粕־╈斉楯챠험ँ奮茅뗀ϻആ凰啧換痰苛逮ଲℕ⢀肇젬푱ᙖ⌊巣發덧ୟᐞ姐惡救痣跟⋟⠿槐裂꺜'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\n",
    "glove_symbols = ''.join([c for c in glove_chars if not c in white_list])\n",
    "glove_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 93259.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#.,=>-?!;):@/\\x89\\n_&([]|*¢$\\x9dª+÷%~£¤}´^¨©«\\\\¬¼¡`{'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw_chars = build_vocab(list(df[\"text\"]))\n",
    "jigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in white_list])\n",
    "jigsaw_symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x89\\n\\x9d'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\n",
    "symbols_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#.,=>-?!;):@/_&([]|*¢$ª+÷%~£¤}´^¨©«\\\\¬¼¡`{'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\n",
    "symbols_to_isolate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 82837.37it/s]\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].progress_apply(lambda x:handle_punctuation(x))\n",
    "# test['comment_text'] = test['comment_text'].progress_apply(lambda x:handle_punctuation(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 192374.23it/s]\n",
      "100%|██████████| 27118/27118 [00:00<00:00, 931083.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 31.17% of vocab\n",
      "Found embeddings for  74.26% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 1238),\n",
       " ('The', 554),\n",
       " ('A', 310),\n",
       " (\"I'm\", 228),\n",
       " ('2', 215),\n",
       " ('In', 160),\n",
       " ('3', 135),\n",
       " ('Û', 133),\n",
       " (\"don't\", 130),\n",
       " (\"it's\", 115)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(list(df['text'].apply(lambda x: x.split())))\n",
    "oov = check_coverage(vocab, glove_embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    x = ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 9008.81it/s]\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].progress_apply(lambda x:handle_contractions(x))\n",
    "# test['comment_text'] = test['comment_text'].progress_apply(lambda x:handle_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 195445.11it/s]\n",
      "100%|██████████| 22131/22131 [00:00<00:00, 864759.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 64.12% of vocab\n",
      "Found embeddings for  94.08% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bioterror', 33),\n",
       " ('prebreak', 30),\n",
       " ('bioterrorism', 28),\n",
       " ('soudelor', 26),\n",
       " ('bestnaijamade', 24),\n",
       " ('disea', 19),\n",
       " ('funtenna', 17),\n",
       " ('crematoria', 15),\n",
       " ('udhampur', 13),\n",
       " ('spos', 9)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(list(df['text'].apply(lambda x: x.split())))\n",
    "oov = check_coverage(vocab, glove_embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lowered_text'] = df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lowered_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/stanford-nlp/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lowered_text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-552c0713eeae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lowered_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moov\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stanford-nlp/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stanford-nlp/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lowered_text'"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(list(df['lowered_text'].apply(lambda x: x.split())))\n",
    "oov = check_coverage(vocab, glove_embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9760"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22414"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5951  , -0.77416 ,  1.579   ,  0.1331  ,  0.82548 ,  0.08094 ,\n",
       "       -0.64546 ,  0.2994  ,  0.93722 , -1.337   , -1.4796  , -2.9713  ,\n",
       "       -2.5895  , -1.4452  , -0.62077 , -1.4272  , -2.2637  , -0.076898,\n",
       "       -0.99475 , -0.56269 , -0.66014 ,  0.38594 ,  0.22842 ,  0.38159 ,\n",
       "        0.13598 ], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings['com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
